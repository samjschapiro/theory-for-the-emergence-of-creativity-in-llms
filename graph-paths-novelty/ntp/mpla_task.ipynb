{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ea02527",
   "metadata": {},
   "source": [
    "## Task 1: Maximum Path Length of Association (MPLA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35964b97",
   "metadata": {},
   "source": [
    "### A. Generate Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcb46db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "NUM_CONCEPTS = 20\n",
    "EDGE_PROB = 0.3 \n",
    "\n",
    "def generate_graph(NUM_CONCEPTS=20, EDGE_PROB=0.3, plot_graph=False):\n",
    "    \"\"\"\n",
    "    Generate a directed graph with a specified number of concepts and edges.\n",
    "    \"\"\"\n",
    "    # Create directed graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Number of concepts\n",
    "    NUM_CONCEPTS = 20\n",
    "    EDGE_PROB = 0.3 \n",
    "    total_edges = 0\n",
    "\n",
    "    # Add concepts to graph\n",
    "    concepts = [f\"C{i+1}\" for i in range(NUM_CONCEPTS)]\n",
    "    G.add_nodes_from(concepts)\n",
    "\n",
    "    # Add edges (relations) to graph\n",
    "    for i in range(NUM_CONCEPTS):\n",
    "        for j in range(i + 1, NUM_CONCEPTS):\n",
    "            if random.random() < EDGE_PROB:\n",
    "                G.add_edge(concepts[i], concepts[j])\n",
    "                total_edges += 1\n",
    "            \n",
    "    if plot_graph:\n",
    "        # Plot graph\n",
    "        pos = nx.spring_layout(G)\n",
    "        nx.draw(G, pos, with_labels=True, node_color='lightblue', edge_color='gray')\n",
    "        plt.title(\"MPLA Concept Graph Example\")\n",
    "        plt.show()\n",
    "\n",
    "    return G, total_edges\n",
    "\n",
    "# Create directed graph\n",
    "G, total_edges = generate_graph(NUM_CONCEPTS=NUM_CONCEPTS, EDGE_PROB=EDGE_PROB, plot_graph=False)\n",
    "concepts = list(G.nodes)\n",
    "relations = list(G.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "257fb913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longest_path(G, source, target):\n",
    "    # Get all simple (non-cyclic) paths from source to target\n",
    "    paths = list(nx.all_simple_paths(G, source=source, target=target))\n",
    "    \n",
    "    # No valid path\n",
    "    if not paths:\n",
    "        return None, 0 \n",
    "    \n",
    "    # Sort paths by length and pick the longest\n",
    "    longest_path = max(paths, key=len)\n",
    "    return longest_path, len(longest_path) - 1  # length in edges\n",
    "\n",
    "\n",
    "def is_valid_path(G, path):\n",
    "    for i in range(len(path) - 1):\n",
    "        if not G.has_edge(path[i], path[i + 1]):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "401ff938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest path from C1 to C10: ['C1', 'C2', 'C3', 'C8', 'C10']\n",
      "Path length: 4\n",
      "Is this a valid path? True\n"
     ]
    }
   ],
   "source": [
    "source = f\"C{1}\"\n",
    "target = f\"C{10}\"\n",
    "\n",
    "path, length = get_longest_path(G, source, target)\n",
    "\n",
    "if path:\n",
    "    print(f\"Longest path from {source} to {target}: {path}\")\n",
    "    print(f\"Path length: {length}\")\n",
    "    print(f\"Is this a valid path? {is_valid_path(G, path)}\")\n",
    "else:\n",
    "    print(f\"No valid path from {source} to {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f307e56d",
   "metadata": {},
   "source": [
    "### B. Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0583e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "DATA_ROOT = \"/Users/samuelschapiro/Desktop/Spiral Works/theory-for-the-emergence-of-creativity-in-llms/graph-paths-novelty/ntp/creativity_data\"\n",
    "\n",
    "NUM_CONCEPTS = 10\n",
    "HASH_STR_LEN = 10\n",
    "EDGE_PROB = 0.3\n",
    "\n",
    "# AI Generated - need to test\n",
    "def build_dataset(num_concepts, edge_prob, hash_str_len):\n",
    "    \"\"\"\n",
    "    Build a dataset of sequences with concepts and their relations.\n",
    "    \"\"\"\n",
    "    G, total_edges = generate_graph(num_concepts, edge_prob, plot_graph=False)\n",
    "    # Create entity vocabulary\n",
    "    concepts = [f\"<C{i+1}>\" for i in range(num_concepts)]\n",
    "    concept_vocab = {concept: i for i, concept in enumerate(concepts)}\n",
    "    \n",
    "    # Generate sequences\n",
    "    train_sequences = []\n",
    "    test_sequences = []\n",
    "    \n",
    "    for _ in tqdm.tqdm(range(1000)):\n",
    "        sequence_length = np.random.randint(5, 15)\n",
    "        sequence = np.random.choice(concepts, size=sequence_length, replace=True)\n",
    "        train_sequences.append(sequence)\n",
    "    \n",
    "    for _ in tqdm.tqdm(range(200)):\n",
    "        sequence_length = np.random.randint(5, 15)\n",
    "        sequence = np.random.choice(concepts, size=sequence_length, replace=True)\n",
    "        test_sequences.append(sequence)\n",
    "\n",
    "    # Create dictionaries for entities\n",
    "    entities_b1_dict = {f\"<b1_{i}>\": f\"<b1_{i}>\" for i in range(num_concepts)}\n",
    "    entities_b2_dict = {f\"<b2_{i}>\": f\"<b2_{i}>\" for i in range(num_concepts)}\n",
    "\n",
    "    return concept_vocab, train_sequences, test_sequences, entities_b1_dict, entities_b2_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1b1149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "DATA_ROOT = \"/Users/samuelschapiro/Desktop/Spiral Works/theory-for-the-emergence-of-creativity-in-llms/graph-paths-novelty/ntp/creativity_data\"\n",
    "\n",
    "# OLD CODE FROM SIBLING DISCOVERY\n",
    "NUM_A = 10\n",
    "NUM_B_PER_A = 1000\n",
    "HASH_STR_LEN = 10\n",
    "def build_dataset(num_a, num_b_per_a, hash_str_len):\n",
    " \n",
    "    entities_a = [\"<a_{}>\".format(i) for i in range(num_a)]\n",
    "\n",
    "    entities_b1 = [\"<b1_{}>\".format(i) for i in range(num_b_per_a * num_a)]\n",
    "    entities_b2 = [\"<b2_{}>\".format(i) for i in range(num_b_per_a * num_a)]\n",
    "\n",
    "    entity_vocab = entities_a + entities_b1 + entities_b2\n",
    "\n",
    "    entities_b1_dict = { entity_a: [entities_b1[i * num_b_per_a + j] for j in range(num_b_per_a)] for i, entity_a in enumerate(entities_a) }\n",
    "    entities_b2_dict = { entity_a: [entities_b2[i * num_b_per_a + j] for j in range(num_b_per_a)] for i, entity_a in enumerate(entities_a) }\n",
    "\n",
    "    # Instead of generating all indices at once, generate hash strings directly\n",
    "    chars = string.ascii_lowercase + string.digits\n",
    "    base = len(chars)\n",
    "    used_hashes = set()  # Keep track of used hash strings\n",
    "    \n",
    "    train_sequences, test_sequences = [], []\n",
    "    for entity_a in tqdm(entities_a):\n",
    "        entities_b1 = entities_b1_dict[entity_a]\n",
    "        entities_b2 = entities_b2_dict[entity_a]\n",
    "        for b1 in tqdm(entities_b1):\n",
    "            for b2 in entities_b2:\n",
    "                # Generate a unique hash string\n",
    "                if hash_str_len == 0:\n",
    "                    hash_str = \"\"\n",
    "                else:\n",
    "                    while True:\n",
    "                        # Generate random digits and convert to hash string\n",
    "                        hash_digits = [random.randint(0, base-1) for _ in range(hash_str_len)]\n",
    "                        hash_str = ''.join(chars[d] for d in hash_digits)\n",
    "                        if hash_str not in used_hashes:\n",
    "                            used_hashes.add(hash_str)\n",
    "                            break\n",
    "                \n",
    "                if np.random.uniform() > 0.005:\n",
    "                    train_sequences.append(form_creativity(hash_str, entity_a, b1, b2))\n",
    "                else:\n",
    "                    test_sequences.append(form_creativity_test(hash_str, entity_a, b1, b2))\n",
    "    \n",
    "    return entity_vocab, train_sequences, test_sequences, entities_b1_dict, entities_b2_dict\n",
    "\n",
    "\n",
    "\n",
    "entity_vocab, train_sequences, test_sequences, entities_b1_dict, entities_b2_dict = build_dataset(NUM_A, NUM_B_PER_A, HASH_STR_LEN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samuelschapiro/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import shutil\n",
    "\n",
    "DATA_ROOT = \"/Users/samuelschapiro/Desktop/Spiral Works/theory-for-the-emergence-of-creativity-in-llms/graph-paths-novelty/ntp/creativity_data\"\n",
    "NUM_A = 10\n",
    "NUM_B_PER_A = 1000\n",
    "HASH_STR_LEN = 10\n",
    "pad_token = \"<|endoftext|>\"\n",
    "\n",
    "def complete(s):\n",
    "    if not s.endswith(\">\"):\n",
    "        s = s + \">\"\n",
    "    if not s.startswith(\"<\"):\n",
    "        s = \"<\" + s\n",
    "    return s\n",
    "\n",
    "def remove_hash(s):\n",
    "    # Remove everything before <q>\n",
    "    q_idx = s.find(\"<q>\")\n",
    "    if q_idx == -1:\n",
    "        return s\n",
    "    return s[q_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size: 50000\n"
     ]
    }
   ],
   "source": [
    "# downsampling train_inferred\n",
    "for training_size in [50000]:\n",
    "    print(f\"training size: {training_size}\")\n",
    "    base_dataset_name = \"sibling.{}.{}.{}.{}\".format(NUM_A, NUM_B_PER_A, HASH_STR_LEN, training_size)\n",
    "    dataset_name = \"sibling.{}.{}.0.{}\".format(NUM_A, NUM_B_PER_A, training_size)\n",
    "    # Copy the base dataset to the new dataset\n",
    "    shutil.copytree(os.path.join(DATA_ROOT, base_dataset_name), os.path.join(DATA_ROOT, dataset_name))\n",
    "\n",
    "    train_sequences = json.load(open(os.path.join(DATA_ROOT, base_dataset_name, \"train.json\"), \"r\", encoding='utf-8'))\n",
    "    # Add the hybrid data to the train_sequences\n",
    "    no_hash_train_sequences = []\n",
    "    for train_sequence in train_sequences:\n",
    "        train_sequence[\"input_text\"] = remove_hash(train_sequence[\"input_text\"])\n",
    "        train_sequence[\"target_text\"] = remove_hash(train_sequence[\"target_text\"])\n",
    "        no_hash_train_sequences.append(train_sequence)\n",
    "    \n",
    "    test_sequences = json.load(open(os.path.join(DATA_ROOT, base_dataset_name, \"test.json\"), \"r\", encoding='utf-8'))\n",
    "    no_hash_test_sequences = []\n",
    "    for test_sequence in test_sequences:\n",
    "        test_sequence[\"input_text\"] = remove_hash(test_sequence[\"input_text\"])\n",
    "        test_sequence[\"target_text\"] = remove_hash(test_sequence[\"target_text\"])\n",
    "        no_hash_test_sequences.append(test_sequence)\n",
    "\n",
    "    with open(os.path.join(DATA_ROOT, dataset_name, \"train.json\"), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(no_hash_train_sequences, f)\n",
    "    with open(os.path.join(DATA_ROOT, dataset_name, \"test.json\"), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(no_hash_test_sequences, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

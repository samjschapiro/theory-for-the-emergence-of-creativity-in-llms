{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samuelschapiro/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "DATA_ROOT = \"/Users/samuelschapiro/Desktop/Spiral Works/theory-for-the-emergence-of-creativity-in-llms/graph-paths-novelty/ntp/creativity_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dicts(entities):\n",
    "    entity2ind = dict()\n",
    "    ind2entity = []\n",
    "    for i in range(len(entities)):\n",
    "        entity = entities[i]\n",
    "        if not (entity in ind2entity):\n",
    "            ind2entity.append(entity)\n",
    "            entity2ind[entity] = len(ind2entity) - 1\n",
    "    return ind2entity, entity2ind\n",
    "\n",
    "def choose(arr, ratio_or_count):\n",
    "    if type(ratio_or_count) == float:\n",
    "        num = round(ratio_or_count*len(arr))\n",
    "    elif type(ratio_or_count) == int:\n",
    "        num = ratio_or_count\n",
    "    else:\n",
    "         assert False\n",
    "    if num >= len(arr):\n",
    "        return arr\n",
    "    rand_inds = np.random.choice(len(arr), num, replace=False).tolist()\n",
    "    return [arr[i] for i in rand_inds]\n",
    "\n",
    "def form_creativity(hash_str, a, b1, b2):\n",
    "    input_text = \"\".join([hash_str, \"<q>\"])\n",
    "    target_text = input_text + \"\".join([b1, b2, a, \"</a>\"])\n",
    "    item = {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text\n",
    "    }\n",
    "    return item\n",
    "\n",
    "\n",
    "def form_creativity_test(hash_str, a, b1, b2):\n",
    "    input_text = \"\".join([hash_str, \"<q>\"])\n",
    "    target_text = input_text + \"\".join([\"</a>\"])  # Placeholder\n",
    "    item = {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text\n",
    "    }\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:06<00:00, 150.61it/s]\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 139.40it/s]\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 132.80it/s]\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 138.67it/s]\n",
      "100%|██████████| 1000/1000 [00:06<00:00, 145.16it/s]\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 125.25it/s]\n",
      "100%|██████████| 1000/1000 [00:06<00:00, 145.79it/s]\n",
      "100%|██████████| 1000/1000 [00:19<00:00, 50.01it/s]\n",
      "100%|██████████| 1000/1000 [00:06<00:00, 143.29it/s]\n",
      "100%|██████████| 1000/1000 [00:06<00:00, 147.13it/s]\n",
      "100%|██████████| 10/10 [01:24<00:00,  8.41s/it]\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(num_a, num_b_per_a, hash_str_len):\n",
    " \n",
    "    entities_a = [\"<a_{}>\".format(i) for i in range(num_a)]\n",
    "\n",
    "    entities_b1 = [\"<b1_{}>\".format(i) for i in range(num_b_per_a * num_a)]\n",
    "    entities_b2 = [\"<b2_{}>\".format(i) for i in range(num_b_per_a * num_a)]\n",
    "\n",
    "    entity_vocab = entities_a + entities_b1 + entities_b2\n",
    "\n",
    "    entities_b1_dict = { entity_a: [entities_b1[i * num_b_per_a + j] for j in range(num_b_per_a)] for i, entity_a in enumerate(entities_a) }\n",
    "    entities_b2_dict = { entity_a: [entities_b2[i * num_b_per_a + j] for j in range(num_b_per_a)] for i, entity_a in enumerate(entities_a) }\n",
    "\n",
    "    # Instead of generating all indices at once, generate hash strings directly\n",
    "    chars = string.ascii_lowercase + string.digits\n",
    "    base = len(chars)\n",
    "    used_hashes = set()  # Keep track of used hash strings\n",
    "    \n",
    "    train_sequences, test_sequences = [], []\n",
    "    for entity_a in tqdm(entities_a):\n",
    "        entities_b1 = entities_b1_dict[entity_a]\n",
    "        entities_b2 = entities_b2_dict[entity_a]\n",
    "        for b1 in tqdm(entities_b1):\n",
    "            for b2 in entities_b2:\n",
    "                # Generate a unique hash string\n",
    "                if hash_str_len == 0:\n",
    "                    hash_str = \"\"\n",
    "                else:\n",
    "                    while True:\n",
    "                        # Generate random digits and convert to hash string\n",
    "                        hash_digits = [random.randint(0, base-1) for _ in range(hash_str_len)]\n",
    "                        hash_str = ''.join(chars[d] for d in hash_digits)\n",
    "                        if hash_str not in used_hashes:\n",
    "                            used_hashes.add(hash_str)\n",
    "                            break\n",
    "                \n",
    "                if np.random.uniform() > 0.005:\n",
    "                    train_sequences.append(form_creativity(hash_str, entity_a, b1, b2))\n",
    "                else:\n",
    "                    test_sequences.append(form_creativity_test(hash_str, entity_a, b1, b2))\n",
    "    \n",
    "    return entity_vocab, train_sequences, test_sequences, entities_b1_dict, entities_b2_dict\n",
    "\n",
    "NUM_A = 10\n",
    "NUM_B_PER_A = 1000\n",
    "HASH_STR_LEN = 10\n",
    "\n",
    "entity_vocab, train_sequences, test_sequences, entities_b1_dict, entities_b2_dict = build_dataset(NUM_A, NUM_B_PER_A, HASH_STR_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 20016\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "vocab = vocab + entity_vocab + [\"<mask>\", \"<sep>\", \"<a>\", \"</a>\", \"<q>\", \"</q>\"]\n",
    "assert len(vocab) == len(set(vocab))\n",
    "print(\"vocab size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 3000\n",
    "test_sequences = choose(test_sequences, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size: 50000\n",
      "50000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "# downsampling train_inferred\n",
    "for training_size in [50000]:\n",
    "    print(f\"training size: {training_size}\")    \n",
    "    dataset_name = \"sibling.{}.{}.{}.{}\".format(NUM_A, NUM_B_PER_A, HASH_STR_LEN, training_size)\n",
    "    os.makedirs(os.path.join(DATA_ROOT, dataset_name), exist_ok=True)\n",
    "    train_sequences_ds = choose(train_sequences, training_size)\n",
    "\n",
    "    # Unique input_text\n",
    "    input_texts = [item[\"input_text\"] for item in train_sequences_ds]\n",
    "    unique_input_texts = list(set(input_texts))\n",
    "\n",
    "    probes = []\n",
    "    for item in choose(train_sequences_ds, test_size):\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1]['type'] = 'train'\n",
    "\n",
    "    for item in test_sequences:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1]['type'] = 'test'\n",
    "\n",
    "    with open(os.path.join(DATA_ROOT, dataset_name, \"train.json\"), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(train_sequences_ds, f)\n",
    "    with open(os.path.join(DATA_ROOT, dataset_name, \"valid.json\"), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(test_sequences, f)\n",
    "    with open(os.path.join(DATA_ROOT, dataset_name, \"test.json\"), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(probes, f)\n",
    "    # add vocab\n",
    "    with open(os.path.join(DATA_ROOT, dataset_name, \"vocab.json\"), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(vocab, f)\n",
    "    # add entities_b1_dict and entities_b2_dict\n",
    "    with open(os.path.join(DATA_ROOT, dataset_name, \"entities_b1_dict.json\"), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(entities_b1_dict, f)\n",
    "    with open(os.path.join(DATA_ROOT, dataset_name, \"entities_b2_dict.json\"), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(entities_b2_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
